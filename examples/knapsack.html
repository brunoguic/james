---
layout: default
---

<div id="examples-knapsack">
    <ol class="breadcrumb">
        <li><a href="/index.html">Home</a></li>
        <li><a href="/examples">Examples</a></li>
        <li class="active">0/1 Knapsack problem</li>
    </ol>
    
    <h2 class="page-header">
        Example 2: The 0/1 knapsack problem
    </h2>
    
    <div class="bs-callout bs-callout-info">
        <h4>
            <i class="fa fa-info-circle"></i>
            Info
        </h4>
        The random descent and parallel tempering metaheuristics are applied to optimize the knapsack contents.
        Don't know what random descent is? Read <a href="/docs/#random-descent">this</a>. Never heard about parallel
        tempering? Read <a href="/docs/#parallel-tempering">this</a>.
    </div>
    <p>
        In this example the 0/1 knapsack problem &mdash; a constrained variable size subset selection
        problem &mdash; is implemented in James. Given a collection of items which each have a weight and profit, the goal
        is to select a subset of these items so that the total profit is maximized without exceeding
        the capacity of the knapsack, i.e. the total weight of all selected items should be
        smaller than or equal to a constant W.
    </p>
    
    <h4 class="page-header">
        Providing the data
    </h4>
    <p>
        First provide the data by implementing the <code>IntegerIdentifiedData</code> interface in <code>KnapsackData</code>.
        Every item is assigned a unique ID in [0, N-1]. The data stores two arrays
        of length N where <code>weights[i]</code> and <code>profits[i]</code> contain
        the weight and profit, respectively, of the item with ID i. These two arrays are specified when
        creating the data and the IDs are automatically inferred to match the array indices. Methods are provided to get all IDs
        (as required by the <code>IntegerIdentifiedData</code> interface) as well as to get the weight or profit
        of an item with a given ID.
    </p>
<pre class="prettyprint">
public class KnapsackData implements IntegerIdentifiedData {

    // weights
    private double[] weights;
    // profits
    private double[] profits;
    // IDs (indices in weight and profit arrays)
    private Set&lt;Integer&gt; ids;
    
    public KnapsackData(double[] weights, double[] profits){
        // store data
        this.weights = weights;
        this.profits = profits;
        // infer IDs: 0..N-1 in case of N items
        // (indices in weight and profit arrays)
        ids = new HashSet&lt;&gt;();
        for(int id=0; id&lt;weights.length; id++){
            ids.add(id);
        }
    }
    
    public Set&lt;Integer&gt; getIDs() {
        return ids;
    }
    
    public double getWeight(int id){
        return weights[id];
    }
    
    public double getProfit(int id){
        return profits[id];
    }

}
</pre>

    <h4 class="page-header">
        Defining the objective
    </h4>
    <p>
        The objective of the knapsack problem is easily defined. We create a class <code>KnapsackObjective</code>
        that implements the <code>Objective</code> interface where the solution and data type are set to
        <code>SubsetSolution</code> and <code>KnapsackData</code> respectively. A given subset solution
        is evaluated by computing the sum of profits of all selected items. This value is to be maximized.
    </p>
<pre class="prettyprint">
public class KnapsackObjective implements Objective&lt;SubsetSolution, KnapsackData&gt;{

    public Evaluation evaluate(SubsetSolution solution, KnapsackData data) {
        // compute sum of profits of selected items
        double totalProfit = solution.getSelectedIDs().stream().mapToDouble(data::getProfit).sum();
        // wrap value in simple evaluation object
        return new SimpleEvaluation(totalProfit);
    }

    public boolean isMinimizing() {
        return false;
    }

}
</pre>
    <p>
        It is advised to also add an efficient delta evaluation to speed up applied neighbourhood searches
        (see <a href="/examples/coresubset2/#delta-eval">example 1B</a>). First, the value (total profit) of the current solution
        is retrieved. Then, the profit of any newly selected items is added and the profit of any removed items
        is subtracted. This yields the value of the neighbour that is obtained when applying the evaluated move
        to the current solution.
    </p>
<pre class="prettyprint">
public Evaluation evaluate(Move move, SubsetSolution curSolution, Evaluation curEvaluation, KnapsackData data) {
    // check move type
    if(!(move instanceof SubsetMove)){
        throw new IncompatibleDeltaEvaluationException("Knapsack objective should be used in combination "
                                            + "with neighbourhoods that generate moves of type SubsetMove.");
    }
    // cast move
    SubsetMove subsetMove = (SubsetMove) move;
    // get current profit
    double value = curEvaluation.getValue();
    // account for added items
    value += subsetMove.getAddedIDs().stream().mapToDouble(data::getProfit).sum();
    // account for removed items
    value -= subsetMove.getDeletedIDs().stream().mapToDouble(data::getProfit).sum();
    // return updated evaluation
    return new SimpleEvaluation(value);
}
</pre>

    <h4 class="page-header">
        Implementing the constraint
    </h4>
    <p>
        To ensure that the knapsack capacity is never exceeded we will add a constraint to our problem. We provide
        a <code>KnapsackConstraint</code> that implements the <code>Constraint</code> interface, with solution
        type <code>SubsetSolution</code> and data type <code>KnapsackData</code>.
        The <code>Constraint</code> interface requires to implement a single method
        <code>Validation validate(SubsetSolution, KnapsackData)</code> that validates a subset solution given our knapsack data.
        In this example, the sum of weights of all selected items is computed and compared to the knapsack capacity. If the
        capacity is not exceeded, the constraint is satisfied.
    </p>
    <p>
        The <code>validate(...)</code> method returns an object of type <code>Validation</code>. The <code>Validation</code> interface
        has a single method <code>passed()</code> that indicates whether the solution passed validation, i.e. whether the constraint
        is satisfied. A predefined implementation of a <code>SimpleValidation</code> is provided that merely wraps a boolean value.
        The constraint below returns such simple validation object.
    </p>
<pre class="prettyprint">
public class KnapsackConstraint implements Constraint&lt;SubsetSolution, KnapsackData&gt; {

    // maximum total weight
    private double maxWeight;
    
    public KnapsackConstraint(double maxWeight){
        this.maxWeight = maxWeight;
    }

    public Validation validate(SubsetSolution solution, KnapsackData data) {
        // compute sum of weights of selected items
        double weight = solution.getSelectedIDs().stream().mapToDouble(data::getWeight).sum();
        // check that maximum weight is not exceeded
        return new SimpleValidation(weight &lt;= maxWeigth);
    }
    
}
</pre>
    <p id="delta-validation">
        Similar to a delta evaluation in an objective, we can provide an efficient delta validation in a constraint.
        This is not required because the <code>Constraint</code> interface includes a default <em>apply-undo</em> behaviour:
        apply the move, perform a full validation by calling <code>validate(solution, data)</code> and undo the move.
        Of course, this is not very efficient so it is advised to provide a custom delta validation whenever possible.
    </p>
<pre class="prettyprint">
public Validation validate(Move move, SubsetSolution curSolution, Validation curValidation, KnapsackData data) {
    // provide delta validation here
}
</pre>
    <p>
        The <code>SimpleValidation</code> type used above only indicates whether the solution passed validation or not.
        Based on this information only, it is not possible to perform an efficient delta evaluation. Therefore, we create
        our own <code>KnapsackValidation</code> that tracks the total weight of a solution, as this is what determines validity.
        It implements the <code>Validation</code> interface with a single required method
        <code>passed()</code> that checks whether the total weight does not exceed the knapsack capacity.
    </p>
<pre class="prettyprint">
public class KnapsackValidation implements Validation {

    private double curWeight;
    private double maxWeight;

    public KnapsackValidation(double curWeight, double maxWeight) {
        this.curWeight = curWeight;
        this.maxWeight = maxWeight;
    }
    
    public double getCurWeight() {
        return curWeight;
    }
    
    public boolean passed() {
        return curWeight &lt;= maxWeight;
    }
    
}
</pre>
    <p>
        Now we can update the constraint to use this custom validation type and include an efficient delta validation.
    </p>
<pre class="prettyprint">
public class KnapsackConstraint implements Constraint&lt;SubsetSolution, KnapsackData&gt; {
    
    // maximum total weight
    private double maxWeight;
    
    public KnapsackConstraint(double maxWeight){
        this.maxWeight = maxWeight;
    }

    public Validation validate(SubsetSolution solution, KnapsackData data) {
        // compute sum of weights of selected items
        double weight = solution.getSelectedIDs().stream().mapToDouble(data::getWeight).sum();
        // return custom validation object
        return new KnapsackValidation(weight);
    }
    
    public Validation validate(Move move, SubsetSolution curSolution, Validation curValidation, KnapsackData data) {
        // check move type
        if(!(move instanceof SubsetMove)){
            throw new IncompatibleDeltaEvaluationException("Knapsack constraint should be used in combination "
                                                + "with neighbourhoods that generate moves of type SubsetMove.");
        }
        // cast move
        SubsetMove subsetMove = (SubsetMove) move;
        // cast current validation object (known to be of the required type as both validate methods return such object)
        KnapsackValidation kVal = (KnapsackValidation) curValidation;
        // extract current sum of weights
        double weight = kVal.getCurWeight();
        // account for added items
        weight += subsetMove.getAddedIDs().stream().mapToDouble(data::getWeight).sum();
        // account for removed items
        weight -= subsetMove.getDeletedIDs().stream().mapToDouble(data::getWeight).sum();
        // return updated validation
        return new KnapsackValidation(weight);
    }
    
}
</pre>
        
    <h4 class="page-header" id="problem-specification">
        Finalizing the problem specification
    </h4>
    <p>
        We are now ready to combine the data, objective and constraint in a <code>SubsetProblem</code>.
        As all subset sizes are allowed, the minimum and maximum size are set to 0 and the total number of items,
        respectively. The data and objective are passed to the problem upon construction and the constraint is
        added afterwards with <code>addMandatoryConstraint(...)</code>. By adding a <i>mandatory</i> constraint,
        applied neighbourhood searches discard all neighbours that violate this constraint.
        The returned best found solution (if any) is then always guaranteed to satisfy the constraint.
    </p>
    <div class="bs-callout bs-callout-info">
        <h4>
            <i class="fa fa-lightbulb-o"></i>
            Mandatory versus penalizing constraints
        </h4>
        <p>
            Instead of a mandatory constraint you may also use a penalizing constraint. Such constraint
            does not cause solutions that violate it to be discarded. Instead, a
            penalty is assigned to the solution's evaluation. This is a more flexible approach compared to mandatory constraints but requires
            to carefully choose the penalties, usually reflecting the severeness of the constraint violation. As there is a tradeoff between
            evaluation and penalties it is not guaranteed that the best found solution of a search will satisfy all penalizing constraints,
            which may or may not be desired. See <a href="/examples/knapsack2/">example 2B</a>.
        </p>
    </div>
<pre class="prettyprint">
// set weights, profits and capacity
double[] weights = ...
double[] profits = ...
double capacity = ...

// create data object
KnapsackData data = new KnapsackData(weights, profits);
// create objective
KnapsackObjective obj = new KnapsackObjective();       
// create constraint
KnapsackConstraint constraint = new KnapsackConstraint(capacity);

// create subset problem (all sizes allowed)
SubsetProblem&lt;KnapsackData&gt; problem = new SubsetProblem&lt;&gt;(obj, data, 0, data.getIDs().size());

// add mandatory constraint
problem.addMandatoryConstraint(constraint);
</pre>
    
    <h4 class="page-header">
        Optimizing the knapsack
    </h4>
    <p>
        We will apply two different local search metaheuristics to optimize the selected knapsack:
        <a href="/docs/#random-descent">random descent</a> (basic) and
        <a href="/docs/#parallel-tempering">parallel tempering</a> (advanced).
        Random descent only accepts modifications of the current solution that yield an improvement so that
        it can easily get stuck in a local optimum. Parallel tempering also
        accepts inferior moves in a controlled way to be able to escape from such local optima.
    </p>
    
    <h5 class="page-header">
        Random descent
    </h5>
    <p>
        First we create the <code>RandomDescent</code> search with a predefined <code>SinglePerturbationNeighbourhood</code> which
        generates moves that add, delete or swap a single (pair of) ID(s) in the current selection. This neighbourhood allows selection
        of variable size subsets.
        The solution type of the search is fixed to <code>SubsetSolution</code>. A maximum runtime is specified as stop criterion and a
        <code>ProgressSearchListener</code> is attached to track the progress of the search (see
        <a href="/examples/coresubset/#search-listener">example 1: core subset selection</a>).
    </p>
<pre class="prettyprint">
// create random descent search with single perturbation neighbourhood
RandomDescent&lt;SubsetSolution&gt; randomDescent = new RandomDescent&lt;&gt;(problem, new SinglePerturbationNeighbourhood());
// set maximum runtime
long timeLimit = ...
randomDescent.addStopCriterion(new MaxRuntime(timeLimit, TimeUnit.SECONDS));
// attach listener
randomDescent.addSearchListener(new ProgressSearchListener());
</pre>
    <p>
        By default, a local search starts from a randomly generated initial solution. However, it may happen that such random
        solution exceeds the knapsack capacity and does not have any valid neighbours so that the search immediately gets stuck.
        To avoid this we specify a custom initial solution. We could simply always start from an empty solution
        but a different approach is taken here to increase the variability of initial solutions.
    </p>
    <div class="bs-callout bs-callout-info">
        <h4>
            <i class="fa fa-lightbulb-o"></i>
            Mandatory versus penalizing constraints (revisited)
        </h4>
        When using a penalizing instead of a mandatory constraint it might not be required to set a custom initial solution that satisfies
        the constraint. If the penalties are carefully chosen, the search may be able to find its way towards solutions within the constraint
        even if it started in a region of the search space where all solutions violate the constraint, because the latter solutions are not
        discarded but penalized. Yet, a penalizing constraint is more difficult to design. See <a href="/examples/knapsack2/">example 2B</a>.
    </div>
<pre class="prettyprint">
// random generator
private static final Random RG = new Random();

// creates a custom initial solution that does not exceed the knapsack capacity
SubsetSolution createInitalSolution(Problem&lt;SubsetSolution&gt; problem, KnapsackData data, double capacity){
    // 1: create random initial solution
    SubsetSolution initialSolution = problem.createRandomSolution();
    // 2: compute current total weight
    double weight = computeSelectionWeight(initialSolution, data);
    // 3: remove random items as long as total weight is larger than the capacity
    while(weight > capacity){
        int id = SetUtilities.getRandomElement(initialSolution.getSelectedIDs(), RG);
        initialSolution.deselect(id);
        weight -= data.getWeight(id);
    }
    // 4: retain random subset to increase variability
    int finalSize = RG.nextInt(initialSolution.getNumSelectedIDs()+1);
    initialSolution.deselectAll(SetUtilities.getRandomSubset(
                                                initialSolution.getSelectedIDs(),
                                                initialSolution.getNumSelectedIDs()-finalSize,
                                                RG)
                                            );
    return initialSolution;
}

// computes the total weight of items selected in the given subset solution
double computeSelectionWeight(SubsetSolution solution, KnapsackData data){
    return solution.getSelectedIDs().stream().mapToDouble(data::getWeight).sum();
}
</pre>
    <div class="bs-callout bs-callout-info">
        <h4>
            <i class="fa fa-lightbulb-o"></i>
            Good to know
        </h4>
        The utility class <code>SetUtilities</code> can be used to sample random items and subsets of any given set.
    </div>
    <p>
        Now we can set a custom initial solution before starting the search.
    </p>
<pre class="prettyprint">
// set custom initial solution that satisfies the constraint
randomDescent.setCurrentSolution(createInitalSolution(problem, data, capacity));
</pre>
    <p>
        Then start the search, get the best found solution when it completes and eventually dispose it to release all resources.
    </p>
<pre class="prettyprint">
// start search (blocks until search terminates)
randomDescent.start();
// print results
if(randomDescent.getBestSolution() != null){
    System.out.println("Items in knapsack: " + randomDescent.getBestSolution().getNumSelectedIDs() + "/" + data.getIDs().size());
    System.out.println("Total profit: " + randomDescent.getBestSolutionEvaluation());
    System.out.println("Total weight: " + computeSelectionWeight(randomDescent.getBestSolution(), data) + "/" + capacity);
} else {
    System.out.println("No valid solution found...");
}
// dispose search
randomDescent.dispose();
</pre>

    <h5 class="page-header" id="parallel-tempering">
        Parallel tempering
    </h5>
    <p>
        The basic random descent search may easily get stuck in a local optimum.
        Advanced searches, such as the <a href="/docs/#parallel-tempering">parallel tempering</a> algorithm,
        provide ways to escape from such local optima so that they can find better solutions. Parallel tempering
        has three parameters: the number of concurrently executed
        <a href="/docs/#metropolis-search">Metropolis searches</a> and the minimum and maximum temperature.
        Especially the temperature bounds should be carefully chosen.
    </p>
<pre class="prettyprint">
// create parallel tempering with single perturbation neighbourhood
double minTemp = 0.001;
double maxTemp = 0.1;
int numReplicas = 10;
ParallelTempering&lt;SubsetSolution&gt; parallelTempering = new ParallelTempering&lt;&gt;(
                                                                    problem,
                                                                    new SinglePerturbationNeighbourhood(),
                                                                    numReplicas, minTemp, maxTemp
                                                                );
</pre>
    <p>
        For parallel tempering it is important to take into account the scale of the objective function
        when setting the minimum and maximum temperature, as the probability to accept an inferior move
        depends on the temperature and the difference in evaluation (delta). If large deltas are common,
        the temperatures should be increased. Else, almost no inferior moves will ever be accepted.
    </p>
    <p>
        In this example, the scale of the evaluations highly depends on the scale of the profits assigned to the items.
        Therefore all temperatures are scaled accordingly by multiplying them with the average profit of all knapsack items.
        Alternatively, we could have normalized all profits to values in [0,1].
    </p>
<pre class="prettyprint">
// scale temperatures according to average profit of knapsack items
double scale = computeAverageProfit(data);
parallelTempering.setTemperatureScaleFactor(scale);
</pre>
    <p>
        The average profit of all knapsack items is easily computed.
    </p>
<pre class="prettyprint">
double computeAverageProfit(KnapsackData data){
    return data.getIDs().stream().mapToDouble(data::getProfit).average().getAsDouble();
}
</pre>
    <p>
        Running the search is now done in exactly the same way as for the random descent algorithm. If you experiment
        with some example input, you will see that the advanced parallel tempering algorithm finds much better solutions
        for the 0/1 knapsack problem as compared to the basic random descent algorithm.
    </p>

    <h4 class="page-header">
        Full source code
    </h4>
    <p>
        The complete source code of this example is available on
        <a href="https://github.com/hdbeukel/james/tree/v{{site.examples-latest-stable}}/james/james-examples/src/main/java/org/jamesframework/examples/knapsack">GitHub</a>,
        including some additional code to read the input from a text file.
        You can also download a <a href="/getstarted/#releases-examples">ZIP file</a> that contains the Java sources of all examples, a compiled
        JAR (including all dependencies) as well as some input files for in case you want to run any of the examples. To run this example, execute
    </p>
<pre class="prettyprint">
$ java -cp james-examples.jar org.jamesframework.examples.knapsack.KnapSack &lt;inputfile&gt; &lt;capacity&gt; &lt;runtime&gt;
</pre>
    </p>
        from the command line. The input file should be a text file in which the first row contains a single number N that indicates
        the number of available knapsack items. The subsequent N rows each contain the profit and weight (in this order) of a single item,
        separated by one or more spaces. The runtime is given in seconds.
    </p>
    <p>
        <ul class="list-unstyled">
            <li><a href="/files/examples/knapsack-100.txt"><i class="fa fa-file-text-o"></i> Example input file 1 (100 items)</a></li>
            <li><a href="/files/examples/knapsack-1000.txt"><i class="fa fa-file-text-o"></i> Example input file 2 (1000 items)</a></li>
        </ul>
    </p>
    
</div>


















