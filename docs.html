---
layout: default
---

<div id="docs">

    <ol class="breadcrumb">
        <li><a href="/index.html">Home</a></li>
        <li class="active">Documentation</li>
    </ol>

    <div id="page-header"></div>

    <div class="row">

        <!-- content -->
        <div class="col-md-9">

            <h2 id="api" class="page-header">
                Javadoc API
            </h2>
            <p>
                A fully specified Javadoc API is provided for the core and extensions module.
            </p>

            <h4 id="latest-stable-api" class="page-header">
                Latest stable API
            </h4>
            <p>
                Consult the latest stable APIs:
                <ul>
                   <li>
                       <a href="/api/core/{{site.core-latest-stable}}">Core module API</a> <small class="text-muted">({{ site.core-latest-stable }})</small>
                   </li>
                   <li>
                       <a href="/api/ext/{{site.extensions-latest-stable}}">Extensions module API</a> <small class="text-muted">({{ site.extensions-latest-stable }})</small>
                   </li>
                </ul>
                Previous versions are found <a href="/docs/api/">here</a>.
            </p>

            <h4 id="snapshot-api" class="page-header">
                Development snapshots
            </h4>
            <p>
                Bleeding edge development snapshot APIs are also available:
                <ul>
                   <li>
                       <a href="/api/core/snapshot">Core module API</a> <small class="text-muted text-danger">({{ site.core-snapshot }})</small>
                   </li>
                   <li>
                       <a href="/api/ext/snapshot">Extensions module API</a> <small class="text-muted text-danger">({{ site.extensions-snapshot }})</small>
                   </li>
                </ul>
                <div class="bs-callout bs-callout-warning">
                    <h4>
                        <i class="fa fa-warning"></i>
                        Warning!
                    </h4>
                    Snapshot APIs may change without notice. Use with care until a stable version is released.
                </div>
            </p>

            <h2 id="problems" class="page-header">
                Problem specification
            </h2>
            <p>
                The JAMES framework strongly separates problem specification from search application so that existing algorithms
                can easily be applied to obtain solutions for newly implemented problems. Each problem has a specific solution
                type and a search creates solutions of this type to solve the problem. The search communicates with the problem to obtain
                random solutions (e.g. used as the default initial solution of a local search) and to evaluate and validate
                constructed solutions.
            </p>
            <img src="/images/diagram.png" class="centered-image" />
            <p>
                Some utilities are provided to split the problem specification into data, an objective, any number of constraints
                and a random solution generator. The objective and constraints are then responsible for evaluating and validating solutions,
                using the data, respectively. Most available optimization algorithms are local searches that repeatedly modify the current
                solution in an attempt to improve it. To use these searches it is necessary to provide at least one neighbourhood, which
                generates moves that are applied to the current solution. The neighbourhood(s) should be compatible with the
                solution type of the problem that is being solved.
            </p>
            <img src="/images/diagram-detailed.png" class="centered-image" />
            <p>
                The core module contains predefined components for subset selection problems, including a predefined solution
                type, a high-level problem specification with a built-in random subset generator, and several subset neighbourhoods.
                Similar components can easily be created for other types of problems. Some are provided in the extensions module.
            </p>

            <h4 id="subset-selection" class="page-header">
                Subset selection
            </h4>
            <p>
                The provided subset selection components require that a unique integer ID is assigned to each item in
                the data set. Any subset selection problem is then translated into selection of a subset of IDs.
                Follow these steps to implement a subset selection problem:
            </p>
            <ol>
                <li>
                    Create a custom data class (e.g. <code>MyData</code>) by implementing the <code>IntegerIdentifiedData</code> interface.
                    This interface defines a single method <code>getIDs()</code> that returns the set of all IDs
                    assigned to an item in the data set. Also add the necessary custom methods to access (properties of)
                    items based on their IDs.
                </li>
                <li>
                    Provide an objective function by implementing the <code>Objective</code> interface. This is a generic interface
                    with two type parameters: the solution and data type.
                    Set the solution type to <code>SubsetSolution</code> and the data type to <code>MyData</code>.
                    Now implement the necessary methods to evaluate solutions using the underlying data and to inform the
                    search whether evaluations are to be minimized or maximized. If desired, an efficient delta evaluation
                    can also be provided, to evaluate a neighbour of the current solution of a local search based on its
                    current evaluation and the applied move.
                </li>
                <li>
                    Specify the constraints (if any) by implementing the <code>Constraint</code> and/or
                    <code>PenalizingConstraint</code> interfaces, to be added as <em>mandatory</em> and/or <em>penalizing</em>
                    constraints to the problem definition. Solutions that do not satisfy all mandatory constraints are discarded.
                    If a penalizing constraint is violated, a penalty is assigned to the solution's evaluation but the solution
                    is still considered as valid.
                    Both interfaces again have a generic solution and data type which should be set to <code>SubsetSolution</code>
                    and <code>MyData</code>, respectively.
                    Efficient delta validations can also be provided (optional).
                </li>
                <li>
                    Combine the data, objective and constraints (if any) in a <code>SubsetProblem</code>.
                    The desired minimum and maximum (or fixed) subset size can be specified. The data and objective are required and
                    set at construction while mandatory and penalizing constraints can be added later using
                    <code>addMandatoryConstraint(c)</code> and <code>addPenalizingConstraint(c)</code>, respectively.
                </li>
            </ol>
            <p>
                The problem definition can be passed to any local search to optimize the selection.
                Several predefined subset neighbourhoods are available which add and/or remove some IDs to/from
                the selection to modify the current solution.
                Note that some searches may require additional parameters or components to be specified.
                Details are best explained through some
                <a href="/examples/#subset-selection">examples</a>.
            </p>

            <h4 id="beyond-subset-selection" class="page-header">
                Beyond subset selection
            </h4>
            <p>
                Other types of problems can also be easily modeled and solved:
            </p>
            <ol>
                <li>
                    Define a custom solution type (e.g. <code>MySolution</code>) by extending the abstract <code>Solution</code> class.
                </li>
                <li>
                    Create a custom data class (e.g. <code>MyData</code>) containing the data required for evaluation, validation and/or random
                    solution generation. The data type is not restricted in any way, i.e. it is not required to implement a specific interface
                    or extend an abstract class. If your problem does not require any data, this step can be skipped.
                </li>
                <li>
                    Implement the objective and constraints (if any) by following the same steps as for a subset selection problem (see above),
                    but now with solution and data type <code>MySolution</code> and <code>MyData</code>, respectively.
                    Also provide a random solution generator, implementing the <code>RandomSolutionGenerator</code> interface,
                    with the same solution and data type. The generated random solutions are for example used to obtain a default
                    initial solution in most local searches.
                    If some component does not use the data (or you did not provide any data), set the data
                    type of the respective class to <code>Object</code> and simply ignore the data argument when implementing
                    the required methods from the respective interface.
                </li>
                <li>
                    Combine the data, objective, constraints (if any) and random solution generator in a <code>GenericProblem</code>
                    with solution and data type <code>MySolution</code> and <code>MyData</code>, respectively. If no data is provided,
                    set the data type to <code>Object</code> and the data argument to <code>null</code> when creating the problem.
                </li>
                <li>
                    Provide at least one implementation of the <code>Neighbourhood</code> interface with solution type <code>MySolution</code>.
                    A neighbourhood generates moves that can be applied to a solution to modify it and undone to go back to the previous solution.
                    All moves implement the <code>Move</code> interface.
                </li>
            </ol>
            <p>
                The composed problem can now be solved with any of the available local searches, using the defined neighbourhood(s) to modify the solution.
                Note that some searches may require additional parameters or components to be specified.
                Examples are provided <a href="/examples/#beyond-subset-selection">here</a>.
            </p>

            <h2 id="algorithms" class="page-header clear">
                Optimization algorithms
            </h2>
            <p>
                All available algorithms that can be used to obtain solutions for the specified problems are listed here.
                Most algorithms are local search metaheuristics, with a few exceptions such as exhaustive
                search and one heuristic specifically designed for subset sampling. Some basic building
                blocks for hybrid (parallel) searches are also provided.
            </p>
            <p>
                If you do not know what a local search metaheuristic is, read
                <a href="http://en.wikipedia.org/wiki/Local_search_(optimization)">this</a>.
            </p>
            <p>
                Most local search algorithms use one ore more neighbourhood(s) to explore the search space, starting
                from a random (or manually specified) initial solution. A neighbourhood defines a set of solutions
                similar to the current solution that can be obtained by applying small changes (moves) to this current
                solution. Such moves are repeatedly applied to the current solution in an attempt to improve it.
                Therefore, the chosen neighbourhoods strongly influence the behaviour of the search algorithms.
            </p>

            <h4 id="random-descent" class="page-header">
                Random descent
            </h4>
            <p>
                Random descent is the most basic local search algorithm (also called stochastic hill climbing).
                In every step, a random neighbour of the current solution is evaluated and accepted as the new
                current solution if and only if it is valid and better than the current solution.
            </p>
            <p>
                Random descent does not terminate internally. Therefore, a stop criterion should be specified, such
                as a maximum number of steps or a maximum runtime.
                By default, the search starts with a random initial solution. Alternatively, a custom initial solution can
                be specified. Any neighbourhood that is compatible with the solution type of the considered problem can be
                applied.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/RandomDescent.html">
                    <i class="fa fa-book"></i>
                    Random descent
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="steepest-descent" class="page-header">
                Steepest descent
            </h4>
            <p>
                Steepest descent (also referred to as hill climbing) is a local search algorithm that always accepts the
                best valid neighbour of the current solution as the new current solution, until no more improvement is
                made.
            </p>
            <p>
                By default, the search starts with a random initial solution. Alternatively, a custom initial solution can
                be specified. Any neighbourhood that is compatible with the solution type of the considered problem can be
                applied.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/SteepestDescent.html">
                    <i class="fa fa-book"></i>
                    Steepest descent
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="tabu-search" class="page-header">
                Tabu search
            </h4>
            <p>
                Tabu Search is based on <a href="#steepest-descent">steepest descent</a> but does not require that an
                improvement is made in every step.
                The best valid non-tabu neighbour of the current solution is always accepted as the new current solution.
                This may allow the search to escape from a local optimum.
            </p>
            <p>
                To avoid repeatedly revisiting the same solutions (i.e. traversing cycles in the solution space)
                the neighbourhood is dynamically modified using a tabu memory.
                This memory keeps track of recently visited solutions, presence of specific features in these solutions and/or
                recently modified features (recently applied moves). Based on this memory, specific solutions are declared
                tabu and are removed from the neighbourhood to favour unvisited regions which may contain new improvements.
            </p>
            <p>
                The search terminates internally if all valid neighbours of the current solution are tabu. However, as
                this may never happen, a stop criterion should preferably be specified to guarantee termination.
                By default, the search starts with a random initial solution. Alternatively, a custom initial solution can
                be specified. Any neighbourhood that is compatible with the solution type of the considered problem can be
                applied.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/tabu/TabuSearch.html">
                    <i class="fa fa-book"></i>
                    Tabu search
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="metropolis-search" class="page-header">
                Metropolis search
            </h4>
            <p>
                Metropolis search is an extension of <a href="#random-descent">random descent</a> where a valid neighbour that
                is no improvement over the current solution may still be accepted as the new current solution. The probability
                to accept such inferior neighbour is proportional to both
                <ol>
                    <li>
                        the difference of its evaluation compared to that of the current solution, and
                    </li>
                    <li>
                        the temperature of the search.
                    </li>
                </ol>
                A higher temperature yields a higher probability to accept inferior moves. Thus, a high temperature allows a lot
                of freedom to escape from local optima, while lower temperatures ease convergence towards an optimum.
            </p>
            <p>
                The basic Metropolis search implementation in JAMES has a fixed temperature. It may therefore not be very useful
                on its own, but it is for example used by the more advanced <a href="#parallel-tempering">parallel tempering</a>
                algorithm. Also, it can provide useful insights in the performance of different temperatures for a specific
                application.
            </p>
            <p>
                Metropolis search does not terminate internally. Therefore, a stop criterion should be specified, such
                as a maximum number of steps or a maximum runtime.
                By default, the search starts with a random initial solution. Alternatively, a custom initial solution can
                be specified. Any neighbourhood that is compatible with the solution type of the considered problem can be
                applied.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/MetropolisSearch.html">
                    <i class="fa fa-book"></i>
                    Metropolis search
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="parallel-tempering" class="page-header">
                Parallel tempering
            </h4>
            <p>
                Parallel tempering (also referred to as Replica Exchange Monte Carlo) is an advanced local search algorithm
                that concurrently runs several <a href="#metropolis-search">Metropolis searches</a> with different temperatures.
                Each of these subsearches repeatedly apply a series of moves on a private solution and the global best solution
                found by any replica is tracked by the main search. The parallel tempering algorithm is somewhat similar to simulated
                annealing but takes advantage of modern multi-core architectures and is generally less sensitive to parameter
                values such as replica temperatures, as compared to e.g. the initial temperature and cooling scheme in simulated
                annealing.
            </p>
            <p>
                When applying the parallel tempering algorithm, the number of replcias and a minimum and maximum temperature
                have to be set. Temperatures assigned to replicas are equally spaced in the specified interval and replicas
                are ordered according to their temperature.
                Between consecutive runs of the Metropolis searches, solutions of neighbouring replicas are swapped
                to push the most promising solutions towards the lowest temperatures, for the sake of convergence, while
                inferior solutions are shifted to hot replicas where they can be extensively modified in an attempt to
                find further improvements. In this way, the hot replicas continuously provide new seeds for the cooler
                replicas.
            </p>
            <p>
                The parallel tempering algorithm does not terminate internally. Therefore, a stop criterion
                should be specified, such as a maximum number of steps or a maximum runtime. Keep in mind that every
                step of parallel tempering consists of a number of steps performed by each Metropolis search.
                By default, every replica starts with an independently generated random initial solution.
                Alternatively, a custom initial solution can be specified (global, or per replica through
                a custom Metropolis search factory).
                Any neighbourhood that is compatible with the solution type of the considered problem can be applied.
            </p>
            <p>
                Note that all Metropolis searches are executed in separate threads to take advantage of multi-core architectures.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/ParallelTempering.html">
                    <i class="fa fa-book"></i>
                    Parallel tempering
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="vnd" class="page-header">
                Variable neighbourhood descent
            </h4>
            <p>
                Variable neighbourhood descent (VND) is an extension of <a href="#steepest-descent">steepest descent</a> which uses a
                series of neighbourhoods N<sub>1</sub>, ..., N<sub>k</sub>. The search starts as a steepest descent using
                neighbourhood N<sub>1</sub>. When this neighbourhood does not contain any improvement (local optimum)
                the other neighbourhoods N<sub>2</sub>, ..., N<sub>k</sub> are
                explored one by one. If an improvement is found in one of these alternative neighbourhoods,
                the algorithm switches back to
                neighbourhood N<sub>1</sub> and continues from the new current solution, else, the search terminates.
            </p>
            <p>
                Combining several neighbourhoods can be beneficial because a local optimum for one neighbourhood is not
                necessarily a local optimum for an other neighbourhood. Often, neighbourhoods of increasing size are chosen
                so that the smallest neighbourhood is most frequently applied, for efficiency, while the larger neighbourhoods
                still offer a way out of local optima of the smaller neighbourhoods.
            </p>
            <p>
                By default, the search starts with a random initial solution. Alternatively, a custom initial solution can
                be specified. Any series of neighbourhoods that are compatible with the solution type of the considered problem
                can be applied.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/vns/VariableNeighbourhoodDescent.html">
                    <i class="fa fa-book"></i>
                    Variable neighbourhood descent
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="rvns" class="page-header">
                Reduced variable neighbourhood search
            </h4>
            <p>
                Reduced variable neighbourhood search (RVNS) is a <a href="#random-descent">random descent</a> based algorithm that uses
                a series of neighbourhoods N<sub>1</sub>, ..., N<sub>k</sub>. It iteratively samples a random neighbour from the
                current neighbourhood N<sub>i</sub> (initially starting with N<sub>1</sub>). If this neighbour is no valid improvement
                over the current solution, a new neighbour is sampled using the next neighbourhood N<sub>i+1</sub> in the subsequent step.
                Else, the neighbour is accepted as the new current solution and the neighbourhood is reset to N<sub>1</sub>.
            </p>
            <p>
                By default, when a neighbour obtained from the last neighbourhood N<sub>k</sub> is not accepted, RVNS cyclically continues with neighbourhood
                N<sub>1</sub> as some neighbourhoods may not yet have been fully explored due to random sampling. If desired,
                this neighbourhood cycling can be disabled in which case the search terminates when one neighbour of the current solution has been
                sampled from every neighbourhood, without finding an improvement.
            </p>
            <p>
                Except when neighbourhood cycling is disabled, reduced variable neighbourhood search never terminates internally.
                Therefore, a stop criterion should be specified, such as a maximum number of steps or a maximum runtime.
                By default, the search starts with a random initial solution. Alternatively, a custom initial solution can
                be specified. Any series of neighbourhoods that are compatible with the solution type of the considered problem
                can be applied.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/vns/ReducedVariableNeighbourhoodSearch.html">
                    <i class="fa fa-book"></i>
                    Reduced variable neighbourhood search
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="vns" class="page-header">
                Variable neighbourhood search
            </h4>
            <p>
                Variable neighbourhood search (VNS) is a more advanced multi-neighbourhood search compared to <a href="#vnd">VND</a> and
                <a href="#rvns">RVNS</a>. It applies a combination of a series of neighbourhoods for random shaking and an arbitrary other
                local search algorithm
                to modify solutions obtained after shaking, in an attempt to find a global improvement. The local search algorithm is applied
                as a black box. More precisely, given a series of shaking neighbourhoods N<sub>1</sub>, ..., N<sub>s</sub> and an arbitrary
                other local search algorithm L, each step of VNS consists of:
                <dl class="dl-horizontal">
                    <dt>Shaking:</dt>
                    <dd>
                        Sample a random neighbour x' of the current solution x, using shaking neighbourhood N<sub>i</sub> (initially, i = 1).
                    </dd>
                    <dt>Local search:</dt>
                    <dd>
                        Apply local search algorithm L with x' as initial solution to obtain x'' (best solution found by L).
                    </dd>
                    <dt>Acceptance:</dt>
                    <dd>
                        If x'' is a valid improvement over x, it is accepted as the new current solution and the search switches back to the
                        first shaking neighbourhood N<sub>1</sub>. Else, x remains the current solution and the next shaking neighbourhood
                        N<sub>i+1</sub> is applied in the subsequent step. If there is no neighbourhood N<sub>i+1</sub> (i &ge; s) the search
                        cyclically continues with N<sub>1</sub>.
                    </dd>
                </dl>
                By default, VNS applies <a href="#vnd">variable neighbourhood descent (VND)</a> as its local search algorithm L, with a distinct
                list of neighbourhoods (i.e. not related to the shaking neighbourhoods of VNS). Alternatively, any custom local search may be applied
                for this purpose.
            </p>
            <p>
                Variable neighbourhood search does not terminate internally. Therefore, a stop criterion
                should be specified, such as a maximum number of steps or a maximum runtime. Keep in mind that one
                step of VNS is composed of shaking and local search and may therefore be quite time consuming, depending
                on the applied local search algorithm L.
                By default, the search starts with a random initial solution. Alternatively, a custom initial solution can
                be specified. Any series of shaking neighbourhoods that are compatible with the solution type of the considered
                problem can be applied.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/vns/VariableNeighbourhoodSearch.html">
                    <i class="fa fa-book"></i>
                    Variable neighbourhood search
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="piped-local-search" class="page-header">
                Piped local search
            </h4>
            <p>
                A piped local search consists of a composition of a series of other local searches S<sub>1</sub>, ..., S<sub>n</sub>.
                These local searches are executed in a pipeline where the best solution found by search S<sub>i</sub>, 1 &le; i &lt; n,
                is used as the initial solution of the next search S<sub>i+1</sub>. The initial solution of search S<sub>1</sub> corresponds
                to the initial solution that has been set for the piped search, if any, else a random initial solution is generated for S<sub>1</sub>.
                The best solution obtained after executing the entire pipeline is eventually returned as the global best solution of the piped search.
            </p>
            <p>
                Note that this is a single step search, meaning that the execution of the entire pipeline is considered to be one step and that
                the piped search terminates when this single step has completed. Moreover, a piped local search can only be started once. After
                its single step has completed, the search automatically disposes itself together with all contained local searches so that it
                can never be restarted.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/PipedLocalSearch.html">
                    <i class="fa fa-book"></i>
                    Piped local search
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="random-search" class="page-header">
                Random search
            </h4>
            <p>
                Random search iteratively samples independent random solutions and checks whether a new valid best solution has been found. It is a very basic algorithm
                and can for example be used for comparison with other searches to assess whether they are able to find better results than random sampling.
            </p>
            <p>
                As random search does not terminate internally, a stop criterion should be specified such as a maximum number of steps (number of randomly sampled solutions)
                or a maximum runtime.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/RandomSearch.html">
                    <i class="fa fa-book"></i>
                    Random search
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="exhaustive-search" class="page-header">
                Exhaustive search
            </h4>
            <p>
                Exhaustive search traverses the entire solution space: every possible solution is evaluated and validated and the best valid solution is selected.
                This algorithm <strong>guarantees</strong> to find the best solution but soon becomes infeasible for many problems of realistic size. However, it
                may be useful for problems with a reasonably small solution space.
            </p>
            <p>
                Traversing the entire solution space is of course very problem specific. The exhaustive search algorithm uses a custom solution iterator for this
                purpose. To apply exhaustive search, such solution iterator must be provided for the considered problem. All solutions generated by this solution
                iterator will be considered. For subset selection, a predefined solution iterator is provided that generates all possible subsets within the desired
                size range.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/exh/ExhaustiveSearch.html">
                    <i class="fa fa-book"></i>
                    Exhaustive search
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="lr-search" class="page-header">
                LR subset search
            </h4>
            <p>
                LR subset search is a greedy heuristic specifically designed for subset selection. It has two parameters L &ge; 0 and R &ge; 0, with L &ne; R, and its
                precise execution depends on whether L &gt; R or R &gt; L:
                <dl class="dl-horizontal">
                    <dt>L &gt; R:</dt>
                    <dd>
                        The search starts with an empty subset and iteratively performs the L best additions followed by the R best removals. The subset size will
                        increase while the search progresses until the desired size is reached.
                    </dd>
                    <dt>R &gt; L:</dt>
                    <dd>
                        All items are initially selected and the search iteratively performs the R best removals followed by the L best additions. The subset size
                        will decrease while the search progresses until the desired size is reached.
                    </dd>
                </dl>
                When small subsets are desired, it is advised to set L &gt; R because then the desired size is more quickly reached when starting from an empty subset,
                while setting R &gt; L is most suited for sampling large subsets (larger than half of the entire set). It is also allowed to specify a custom initial
                solution before starting the search.
            </p>
            <p>
                The LR subset search terminates as soon as the desired subset size is reached. If a minimum and maximum allowed size are specified, the search terminates
                when the entire valid subset size range has been explored.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/LRSubsetSearch.html">
                    <i class="fa fa-book"></i>
                    LR subset search
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h4 id="basic-parallel-search" class="page-header">
                Basic parallel search
            </h4>
            <p>
                The basic parallel search algorithm concurrently runs a (possibly heterogeneous) collection of other searches and keeps track of the global best
                solution found by any subsearch. It provides an easy way to create a parallel hybrid search, for example containing different algorithms or multiple
                instances of the same local search with different initial solutions (multi-start). All searches run in a separate thread so that on multicore machines
                (with a sufficient amount of cores) searches will be executed in parallel.
            </p>
            <p>
                <a href="/api/core/{{ site.core-latest-stable }}/index.html?org/jamesframework/core/search/algo/BasicParallelSearch.html">
                    <i class="fa fa-book"></i>
                    Basic parallel search
                </a>
                <small class="text-muted">
                    (API version: <span>{{ site.core-latest-stable }}</span>)
                </small>
            </p>

            <h2 id="github-wiki" class="page-header">
                GitHub wiki
            </h2>

            <p>
                Additional information about the architecture of the JAMES framework is available on the
                <a href="https://github.com/hdbeukel/james/wiki">GitHub wiki pages</a>. The wiki includes
                several UML diagrams and gives a detailed description of the lifecycle of a search. You can
                also browse the source code on GitHub.
            </p>

        </div><!-- end of content -->

        <!-- sidebar -->
        <div class="col-md-3 hidden-xs hidden-sm">
            <div class="bs-docs-sidebar" data-spy="affix" data-offset-top="150" data-offset-bottom="150">
                <ul class="nav nav-stacked">
                    <li class="active sidebar-lead">
                        <a href="#page-header">Documentation</a>
                    </li>
                    <li>
                        <a href="#api">Javadoc API</a>
                        <ul class="nav nav-stacked">
                            <li><a href="#latest-stable-api">Latest stable API</a></li>
                            <li><a href="#snapshot-api">Development snapshots</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#problems">Problem specification</a>
                        <ul class="nav nav-stacked">
                            <li><a href="#subset-selection">Subset selection</a></li>
                            <li><a href="#beyond-subset-selection">Beyond subset selection</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#algorithms">Optimization algorithms</a>
                        <ul class="nav nav-stacked">
                            <li><a href="#random-descent">Random descent</a></li>
                            <li><a href="#steepest-descent">Steepest descent</a></li>
                            <li><a href="#tabu-search">Tabu search</a></li>
                            <li><a href="#metropolis-search">Metropolis search</a></li>
                            <li><a href="#parallel-tempering">Parallel tempering</a></li>
                            <li><a href="#vnd">Variable neighbourhood descent</a></li>
                            <li><a href="#rvns">Reduced variable neighbourhood search</a></li>
                            <li><a href="#vns">Variable neighbourhood search</a></li>
                            <li><a href="#piped-local-search">Piped local search</a></li>
                            <li><a href="#random-search">Random search</a></li>
                            <li><a href="#exhaustive-search">Exhaustive search</a></li>
                            <li><a href="#lr-search">LR subset search</a></li>
                            <li><a href="#basic-parallel-search">Basic parallel search</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#github-wiki">GitHub wiki</a>
                    </li>
                </ul>
                <a class="back-to-top" href="#">Back to top</a>
            </div>
        </div><!-- end of sidebar -->

</div>
