---
layout: default
---

<div id="examples">
    <ol class="breadcrumb">
        <li><a href="/index.html">Home</a></li>
        <li class="active">Examples</li>
    </ol>

    <div class="row">
    
        <!-- content -->
        <div class="col-lg-9">

            <h1 id="page-header" class="page-header">
                Examples
            </h1>
            <p>
                Step-by-step implementation guides are provided here for a series of examples. The full source code
                of all examples is bundled in the James examples module. You can browse the code on
                <a href="https://github.com/hdbeukel/james/tree/v{{site.examples-latest-stable}}/james/james-examples">GitHub</a>
                or download a <a href="/getstarted/#releases-examples">ZIP file</a> that contains all
                Java sources, a compiled JAR (including all dependencies) as well as example input files for in
                case you would like to run any of the examples.
            </p>
            
            <h2 id="subset-selection" class="page-header">
                Subset selection problems
            </h2>
            <p>
                These examples show how to model a subset selection problem
                and how to apply a variety of searches to find good solutions.
            </p>
            
            <h4 class="page-header example-header">
                Example 1: Core subset selection
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Basic fixed size subset selection.
                <span class="algo">
                    Algorithm: random descent with a predefined subset neighbourhood (single swap).
                </span>
            </p>
            <p>
                The core subset selection problem originates from the field of crop science. Given a large collection
                of crop varieties, the goal is to select a maintainable subset that represents
                the diversity of the entire collection as much as possible. There are several approaches
                to this problem, with distinct objective functions.
            </p>
            <p>
                This example discusses the implementation of a simplified core subset selection problem in James.
                It is assumed that a distance matrix is available in which the dissimilarity of any pair of crop
                varieties is expressed. The goal is to select a subset of fixed size with maximum average distance
                between all pairs of selected items.
            </p>
            <p>
                The basic <a href="/docs/#random-descent">random descent</a> algorithm is applied to sample a core subset,
                using a predefined subset neighbourhood that swaps a single selected and unselected ID to modify the
                current solution.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/coresubset"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h4 class="page-header example-header">
                Example 1B: Core subset selection (2)
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-half-full"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Efficient delta evaluation in neighbourhood searches.
                <span class="algo">
                    Algorithm: random descent with a predefined subset neighbourhood (single swap).
                </span>
            </p>
            <p>
                When a neighbourhood search evaluates a neighbour of the current solution, both solutions are usually
                very similar. It is therefore often possible to evaluate such neighbour based on the current solution,
                its evaluation and the applied move (changes) using fewer computations
                as compared to a full evaluation of the neighbour.
            </p>
            <p>
                This example demonstrates how to implement such efficient delta evaluation for the same problem
                and objective from example 1.
                The same basic <a href="/docs/#random-descent">random descent</a> algorithm is applied to sample a core subset,
                using the predefined single swap neighbourhood.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/coresubset2"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h4 class="page-header example-header">
                Example 1C: Core subset selection (3)
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Custom evaluation objects with metadata for delta evaluation.
                <span class="algo">
                    Algorithms: random descent and parallel tempering with a predefined
                    subset neighbourhood (single swap).
                </span>
            </p>
            <p>
                Various objective functions have been proposed for the core subset selection problem.
                In this example, the average distance from every selected item to the closest other selected
                item is maximized, instead of the average distance between all pairs. To be able to provide
                an efficient delta evaluation for this objective, a custom evaluation object is constructed
                that stores additional metadata.
            </p>
            <p>
                The <a href="/docs/#random-descent">random descent</a> and
                <a href="/docs/#parallel-tempering">parallel tempering</a> algorithms are applied to sample a core subset,
                using a predefined subset neighbourhood that swaps a single selected and unselected ID to modify the
                current solution.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/coresubset3"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h4 class="page-header example-header">
                Example 2: The 0/1 knapsack problem
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Variable size subset selection with a mandatory constraint.
                <span class="algo">
                    Algorithms: random descent and parallel tempering
                    with a predefined subset neighbourhood (single perturbation).
                </span>
            </p>
            <p>
                The 0/1 knapsack problem is a contrained variable size subset selection problem. The input consists of a list of items
                which each have a fixed weight and profit. The goal is to select a subset of these items with maximal total profit where
                the total weight does not exceed a given knapsack capacity. The latter requirement is imposed using a mandatory constraint
                so that invalid solutions are discarded during search.
            </p>
            <p>
                The <a href="/docs/#random-descent">random descent</a> and <a href="/docs/#parallel-tempering">parallel tempering</a>
                algorithms are applied to optimize the selection, using a predefined subset neighbourhood that adds, deletes or
                swaps a (pair of) ID(s) to modify the current selection.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/knapsack"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h4 class="page-header example-header">
                Example 2B: The 0/1 knapsack problem (2)
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-half-full"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Variable size subset selection with a penalizing constraint.
                <span class="algo">
                    Algorithm: parallel tempering with a predefined subset neighbourhood (single perturbation).
                </span>
            </p>
            <p>
                In this example, the 0/1 knapsack problem is solved using a penalizing instead of a mandatory constraint.
                Such penalizing constraint does not cause solutions that violate it to be discarded, but assigns a penalty
                to such solution's evaluation. This allows the search to cross regions of the search space with solutions
                outside the constraint.
            </p>
            <p>
                The <a href="/docs/#parallel-tempering">parallel tempering</a>
                algorithm is applied to optimize the selection, using a predefined subset neighbourhood that adds, deletes or
                swaps a (pair of) ID(s) to modify the current selection.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/knapsack2"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h4 class="page-header example-header">
                Example 3: The maximum clique problem
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-half-full"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Variable size subset selection with a custom greedy neighbourhood.
                <span class="algo">
                    Algorithms: random descent and variable neighbourhood search.
                </span>
            </p>
            <p>
                The maximum clique problem originates from graph theory and has many applications in e.g. social networks, bioinformatics
                and computational chemistry. The goal is to find a clique of maximum size in a given graph. Such clique consists of a subset
                of vertices which are all connected to each other (a complete subgraph).
            </p>
            <p>
                The <a href="/docs/#random-descent">random descent</a> and <a href="/docs/#vns">variable neighbourhood search</a>
                algorithms are applied to search for a maximum clique. The algorithms start from an empty clique and use a custom greedy
                neighbourhood that always adds a single vertex which is connected to all vertices in the current clique. The variable
                neighbourhood search uses additional shaking neighbourhoods to escape from local optima.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/clique"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h2 id="beyond-subset-selection" class="page-header">
                Beyond subset selection
            </h2>
            <p>
                These examples show how to implement and solve other types of problems besides subset selection.
            </p>
            
            <h4 class="page-header example-header">
                Example 4: The travelling salesman problem
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Basic custom problem specification.
                <span class="algo">
                    Algorithms: random descent and parallel tempering.
                </span>
            </p>
            <p>
                The travelling salesman problem consists of finding the shortest
                round trip through a list of given cities. Each city is to be visited
                exactly once and the trip has to end in the same city where the tour started.
                This example considers the <em>symmetric</em> TSP where the travel distance
                from <var>a</var> to <var>b</var> is always the same as that from <var>b</var>
                to <var>a</var>. The goal is to create a permutation (ordered list) of all cities
                so that visiting the cities in this specific order yields the lowest possible
                total travel distance.
            </p>
            <p>
                The <a href="/docs/#random-descent">random descent</a> and
                <a href="/docs/#parallel-tempering">parallel tempering</a>
                algorithms are applied to optimize the round trip using a custom
                neighbourhood that reverses subseries of cities
                in given TSP solutions. This type of move is often referred to as a 2-opt move
                in the context of TSP.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/tsp"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h4 class="page-header example-header">
                Example 4B: The travelling salesman problem (2)
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Design of reusable components for permutation problems.
                <span class="algo">
                    Algorithms: random descent and parallel tempering.
                </span>
            </p>
            <p>
                This example reconsiders the travelling salesman problem. As this is actually a permutation
                problem, generic components are designed that can be reused for any permuation problem, similar
                to the subset selection components already available in the core module. The necessary TSP data
                and objective are then plugged in.
            </p>
            <p>
                All generic permutation problem components designed in this example have been made available in the
                <a href="/getstarted/#modules">James extensions module</a>.
            </p>
            <p>
                The <a href="/docs/#random-descent">random descent</a> and
                <a href="/docs/#parallel-tempering">parallel tempering</a>
                algorithms are applied to optimize the round trip with a generic
                neighbourhood that reverses subsequences in given permutations,
                one of the permutation neighbourhoods provided in the extensions module.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/tsp2"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
        </div><!-- end of content -->      
    
</div>






