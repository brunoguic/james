---
layout: default
---

<div id="examples">
    <ol class="breadcrumb">
        <li><a href="/index.html">Home</a></li>
        <li class="active">Examples</li>
    </ol>

    <div class="row">
    
        <!-- content -->
        <div class="col-lg-9">

            <h1 id="page-header" class="page-header">
                Examples
            </h1>
            <p>
                Step-by-step implementation guides are provided here for a series of examples. The full source code
                of all examples is bundled in the James examples module. You can browse the code on
                <a href="https://github.com/hdbeukel/james/tree/v{{site.examples-latest-stable}}/james/james-examples">GitHub</a>
                or download a <a href="/getstarted/#releases-examples">ZIP file</a> that contains all
                Java sources, a compiled JAR (including all dependencies) as well as example input files for in
                case you would like to run any of the examples.
            </p>
            
            <h2 id="subset-selection" class="page-header">
                Subset selection problems
            </h2>
            <p>
                These examples show how to model a subset selection problem
                and how to apply a variety of searches to find good solutions.
            </p>
            
            <h4 class="page-header example-header">
                Example 1: Core subset selection
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Basic fixed size subset selection.
                <span class="algo">
                    Algorithm: random descent with a predefined subset neighbourhood (single swap).
                </span>
            </p>
            <p>
                The core subset selection problem originates from the field of crop science. Given a large collection
                of crop varieties, the goal is to select a maintainable subset that represents
                the diversity of the entire collection as much as possible. There are several approaches
                to this problem, with distinct objective functions.
            </p>
            <p>
                This example discusses the implementation of a simplified core subset selection problem in James.
                It is assumed that a distance matrix is available in which the dissimilarity of any pair of crop
                varieties is expressed. The goal is to select a subset of fixed size with maximum average distance
                between all pairs of selected items.
            </p>
            <p>
                The basic <a href="/docs/#random-descent">random descent</a> algorithm is applied to sample a core subset,
                using a predefined subset neighbourhood that swaps a single selected and unselected ID to modify the
                current solution.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/coresubset"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h4 class="page-header example-header">
                Example 1B: Core subset selection (2)
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-half-full"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Efficient delta evaluation in neighbourhood searches.
                <span class="algo">
                    Algorithm: random descent with a predefined subset neighbourhood (single swap).
                </span>
            </p>
            <p>
                When a neighbourhood search evaluates a neighbour of the current solution, both solutions are usually
                very similar. It is therefore often possible to evaluate such neighbour based on the current solution,
                its evaluation and the applied move (changes) using fewer computations
                as compared to a full evaluation of the neighbour.
            </p>
            <p>
                This example demonstrates how to implement such efficient delta evaluation for the same problem
                and objective from example 1.
                The same basic <a href="/docs/#random-descent">random descent</a> algorithm is applied to sample a core subset,
                using the predefined single swap neighbourhood.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/coresubset2"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h4 class="page-header example-header">
                Example 1C: Core subset selection (3)
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Custom evaluation objects with metadata for delta evaluation.
                <span class="algo">
                    Algorithms: random descent and parallel tempering with a predefined
                    subset neighbourhood (single swap).
                </span>
            </p>
            <p>
                Various objective functions have been proposed for the core subset selection problem.
                In this example, the average distance from every selected item to the closest other selected
                item is maximized, instead of the average distance between all pairs. To be able to provide
                an efficient delta evaluation for this objective, a custom evaluation object is constructed
                that stores additional metadata.
            </p>
            <p>
                The <a href="/docs/#random-descent">random descent</a> and
                <a href="/docs/#parallel-tempering">parallel tempering</a> algorithms are applied to sample a core subset,
                using a predefined subset neighbourhood that swaps a single selected and unselected ID to modify the
                current solution.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/coresubset3"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h4 class="page-header example-header">
                Example 2: The 0/1 knapsack problem
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Variable size subset selection with a mandatory constraint.
                <span class="algo">
                    Algorithms: random descent and parallel tempering
                    with a predefined subset neighbourhood (single perturbation).
                </span>
            </p>
            <p>
                The 0/1 knapsack problem is a contrained variable size subset selection problem. The input consists of a list of items
                which each have a fixed weight and profit. The goal is to select a subset of these items with maximal total profit where
                the total weight does not exceed a given knapsack capacity. The latter requirement is imposed using a mandatory constraint
                so that invalid solutions are discarded during search.
            </p>
            <p>
                The <a href="/docs/#random-descent">random descent</a> and <a href="/docs/#parallel-tempering">parallel tempering</a>
                algorithms are applied to optimize the selection, using a predefined subset neighbourhood that adds, deletes or
                swaps a (pair of) ID(s) to modify the current selection.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/knapsack"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h4 class="page-header example-header">
                Example 2B: The 0/1 knapsack problem (2)
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-half-full"></i>
                        <i class="fa fa-star-o"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Variable size subset selection with a penalizing constraint.
                <span class="algo">
                    Algorithm: parallel tempering with a predefined subset neighbourhood (single perturbation).
                </span>
            </p>
            <p>
                In this example, the 0/1 knapsack problem is solved using a penalizing instead of a mandatory constraint.
                Such penalizing constraint does not cause solutions that violate it to be discarded, but assigns a penalty
                to such solution's evaluation. This allows the search to cross regions of the search space with solutions
                outside the constraint.
            </p>
            <p>
                The <a href="/docs/#parallel-tempering">parallel tempering</a>
                algorithm is applied to optimize the selection, using a predefined subset neighbourhood that adds, deletes or
                swaps a (pair of) ID(s) to modify the current selection.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/knapsack2"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h4 class="page-header example-header">
                Example 3: The maximum clique problem
                <span class="hidden-xs">
                    <span class="pull-right label label-warning">
                        Difficulty:
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star"></i>
                        <i class="fa fa-star-half-full"></i>
                        <i class="fa fa-star-o"></i>
                    </span>
                </span>
            </h4>
            <p class="lead">
                Variable size subset selection with a custom greedy neighbourhood.
                <span class="algo">
                    Algorithms: random descent and variable neighbourhood search.
                </span>
            </p>
            <p>
                The maximum clique problem originates from graph theory and has many applications in e.g. social networks, bioinformatics
                and computational chemistry. The goal is to find a clique of maximum size in a given graph. Such clique consists of a subset
                of vertices which are all connected to each other (a complete subgraph).
            </p>
            <p>
                The <a href="/docs/#random-descent">random descent</a> and <a href="/docs/#vns">variable neighbourhood search</a>
                algorithms are applied to search for a maximum clique. The algorithms start from an empty clique and use a custom greedy
                neighbourhood that always adds a single vertex which is connected to all vertices in the current clique. The variable
                neighbourhood search uses additional shaking neighbourhoods to escape from local optima.
            </p>
            <p>
                <a class="btn btn-default" href="/examples/clique"><i class="fa fa-chevron-right"></i> View example</a>
            </p>
            
            <h2 id="beyond-subset-selection" class="page-header">
                Beyond subset selection
            </h2>
            <p>
                These examples show how to implement and solve other types of problems.
            </p>
            
            <div class="bs-callout bs-callout-info">
                <h4>
                    <i class="fa fa-info-circle"></i>
                    Info
                </h4>
                Examples coming soon.
            </div>
            
        </div><!-- end of content -->      
    
</div>
